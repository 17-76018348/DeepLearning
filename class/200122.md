# 0122

목차

1. Polynomial regression
2. likelihood
3. MLE/Map

---

- Linear reg에 poloynomial reg가 포함된다고 생각하는사람도 있음
- polynomial 의 핵심 몇차인지가 중요

---

1. polynomial이 linear에서 차이가크게 없다
2. overfitting 위해 차원

---

- 오버피팅 원인
    - 노이즈까지 학습해서
    - 너무 능력이 좋은 모델 쓴거임
- 100차원에는 2차원도 포함되어있는데
    - 이건 나머지 파라미터가 0이어야되는데 노이즈때문에 0이 아님
- 오버피팅되면 loss가 0이 된다
- 실제 트레이닝셋 에러보다 테스트 에러가 높다하면 오버피팅이 되는거임
    - 노이즈까지 외워버린거임
    - 하지만 일단 기본적으로 트레이닝보다 테스트가 높긴함
- 어느모델에서 오버피팅이 되는지 몰라서 속칭 노가다로 해야함
- 오버피팅되면 파라미터가 기하급수적으로 높아짐
- 오버피팅 막기위해 데이터셋이 많아야된다
    - 데이터셋 양이 장땡이다
    - 데이터셋이 많으면 모델이 아무리 성능이 좋아도 오버피팅 해결할수있다
- augmentation
    - 실제 원래 가지고 있던 값에 대해서 노이즈를 섞어서 추가한다
    - 하지만 너무 많이 하면 그 타입을 하나로 인식해서 오버피팅 가능성 있음
    - 그니까 적당히 해라
    - 사진에 대해서 shifting 회전 등등 다양한 전략 사용 가능
- weight decay
    - loss에다가 페널티를 더함
    - 유클리디안 디스턴스가 커지면 그에 대해서 페널티를 주는것임
    - 어떤 벡터의 절댓값이 커지면 그에 대해 페널티가 생긴거임
    - 벡터의 절댓값을 얼마나 반영할지가 람다임
    - 람다도 사람이 직접 잡고 이거에 따라 성능 달라짐
    - 보통 람다 정할때 로그 씌움
        - 이유는 값이 작으면 어느정도 플랫한 값으로 만들어줌
    - 람다를 잘못 잡으면 파라미터가 조금만 커져도 막아버림
    - 람다가 마이너스 무한대라면 거의 안잡는거임 페널티가 의미가 없는거임
    - 최적의 람다를 잡으면 파라미터 커지는걸 막아주는 역할을 한다

---

### MLE MAP

- statQuest 좋은 사이트임

---

Probability vs Likelihood

- probability
    - 관심있는 확률 | distribution
- likelihood
    - 하나를 뽑을때 어떤 distribution에서 나올 확률 이 높냐
    - 데이터셋이 각각의 모델에서 나올 확률을 측정
    - 데이터 가질고 있을때 어떤 배경에서 나올 확률이 높냐
    - 이걸 linear reg에 적용하면 데이터를 가지고 있을때 어떤 파라미터에서 가장 높은 확률을 가지냐
- pro하고 likeli 하고 계산하는 식이 같음
    - 그럴수밖에 없는게 likelihood 데이터가 distri에서 뽑일 확률임
- mean을 바꿔가면서확률을 구하는게 likelihood  이거를 최대화하는게 MLE 임
- mean을 고정시키고std을 바꿔가면서 할거임
    - 근데 이거는 크게 의미가 없는게 0일때가 가장 좋음
- iid 뽑을 때마다 독립적으로 뽑히는거임
    - 뽑을때마다 likelihood 곱으로 표현할수있다
    - (증명 해보기)
    - A 교 B = A 곱 ㅠ
- likelihood를 전부 곱한다는건 전부의 의견을 듣는거임
- 뮤를 구하는게 하나씩에서 뽑고 각각을 다 곱하면 전체의 likelihood가 나옴

---

시험 볼거임

어떻게 데이터 전체에 대해서 MLE을 뽑을 수 있는지 손으로 계산

- normal distirbution에서 나왔다고 가정하고
- log를 사용하는데 그 이유는 곱을 합으로 바꿈으로써 미적분 등 다양한 계산이 가능해지는거임
- 데이터의 평균이 MLE에서 최적의뮤값이 되는거임
- 시그마(분산) 또한 마찬가지임
- 즉 데이터의 mean과 std를 구하면 normal distributon의 mean 과 std가 된다
- 톰 미첼

---

### Map

- 확률의 큰장점은 시행을 많이하면 물리적 수학적을 제외하고도 알아낼수도있음
- MLE에서 prior추가된게 Map
- 감마를 끼워넣음( prior 를 강조하는 방법)
    - 똑같은 감마를 넣는데 10배 100배를 하면 영향력이 커짐
    - prior를 강조하는걸 배수로서 강조한다
- 즉 linear reg에서 세타의 범위를 미리 생각해서 초기값을 영향을 미치는건 Map임
- 지금까지 한건 MLE
- MAP에 학습에서 페널티를 주는것 또한 Map라고 볼수도있음
- 그렇다면 막연하게 사전지식을 넣는게 좋은거냐?
    - 잘못된 prior를 넣게되면 오히려 악영향
    - 적당한 값을 넣어야함

---

MLE란 linear reg에서

모든 파라미터가 uniform distribution 즉 모든 상황에 공평하게 있는거임

prior 가 상수  

MAP에서모든 파라미터가 uniform distri라고 한다면 MLE와 다를게 없다

prior를 weight라고 생각하기도 한다

---

### logistic reg

linear한게 계속 쌓이면 결국 linear임 즉 의미가 없음

레이어 하나랑 차이가 없음 

그래서 어떤걸 넣어줄지가 관건임

중간에 non linear한걸 넣어줘야 다음 스택을 쌓는것이 의미가있음

---

sigmoid

아웃라이어를 제거하지않는다면 모델이 아웃라이어하고 코스트가 커져서 러닝을 망쳐버린다

그래서 이 시그모이드가 아웃라이어를 제외해줄수 있다

값이 중앙에서 멀어지면 많이 벗어나도y값에선 큰 차이가 없다 아웃라이어의 영향력을 줄여줄수있다 

또한 0에서 1의 값을 가진다 즉 확률로 접근하기 좋아진다

logistic 에서 는 cost가 볼록한 모양이 안나온다

즉 원래의 loss를 쓸수가없다 

mean square error 대신에 그래서 다른걸 사용한다